import torch
import torch.nn as nn

class Phi4MMForClassification(nn.Module):
    def __init__(self, base_model, num_classes):
        super().__init__()
        self.backbone = base_model
        hidden_size = base_model.config.hidden_size
        self.classifier = nn.Linear(hidden_size, num_classes)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, input_ids=None, attention_mask=None, **kwargs):
        # 1. Get the transformer hidden states
        outputs = self.backbone.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)
        hidden_states = outputs.last_hidden_state  # (B, T, H)
        
        # 2. Use last token or mean pooling
        pooled = hidden_states.mean(dim=1)  # (B, H)
        
        # 3. Feed into classifier
        logits = self.classifier(pooled)  # (B, num_classes)
        
        # 4. Return softmax probabilities
        probs = self.softmax(logits)
        return probs
